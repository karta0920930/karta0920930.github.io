import requests
from bs4 import BeautifulSoup
import pandas as pd
import datetime
import os
import json
import warnings
from bs4 import BeautifulSoup, XMLParsedAsHTMLWarning
# =========================
# 1. åŸºæœ¬è¨­å®š
# =========================
OUTPUT_DIR = "data"
if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)

TODAY_STR = datetime.datetime.today().strftime("%Y-%m-%d")
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

# =========================
# 2. çˆ¬å–å°ç£æ–°è (æ”¹ç”¨ Yahoo æ–°è - ä¿éšªé—œéµå­—)
# =========================
def get_taiwan_news():
    print("ğŸ” å˜—è©¦æŠ“å–å°ç£ä¿éšªæ–°è (Yahoo)...")
    # æŠ“å– Yahoo æœå°‹ã€Œä¿éšªã€çš„æœ€æ–°æ–°èçµæœ
    url = "https://tw.news.yahoo.com/search?p=%E4%BF%9D%E9%9A%AA&fr=news"
    articles = []
    TW_BLACKLIST = ["ä¿éšªå¥—",  "å»£å‘Š", "ç™¼è²¡"]
    try:
        response = requests.get(url, headers=HEADERS, timeout=15)
        soup = BeautifulSoup(response.text, "html.parser")
        
        # Yahoo æ–°èæ¨™é¡Œé€šå¸¸åœ¨ <h3> ä¸­
        items = soup.find_all("h3")
        for item in items:
            title_tag = item.find("a")
            if title_tag:
                raw_title = title_tag.get_text(strip=True)
                href = title_tag.get('href')
                
                # --- æŠ“é‡é»é‚è¼¯ 1ï¼šç§»é™¤ä¾†æºå­—æ¨£ ---
                # å¾ˆå¤šæ¨™é¡Œæœƒé•·é€™æ¨£ï¼šã€Œä¿éšªæ¥­ç²åˆ©å‰µæ–°é«˜ - Yahooå¥‡æ‘©æ–°èã€
                # æˆ‘å€‘åªå– '-' æˆ–æ˜¯ '|' ä¹‹å‰çš„å…§å®¹
                clean_title = raw_title.split(' - ')[0].split(' | ')[0].split(' (')[0]

                # --- æŠ“é‡é»é‚è¼¯ 2ï¼šéæ¿¾ ---
                if any(word in clean_title for word in TW_BLACKLIST):
                    continue

                if href and len(clean_title) > 10:
                    if not href.startswith('http'):
                        href = "https://tw.news.yahoo.com" + href
                    
                    articles.append({
                        "title": clean_title, # ä½¿ç”¨æ¸…ç†å¾Œçš„é‡é»æ¨™é¡Œ
                        "link": href,
                        "date": TODAY_STR,
                        "source": "å°ç£æ–°è"
                    })
            
            # --- æ•¸é‡é™åˆ¶ï¼šé™ 10 å‰‡ ---
            if len(articles) >= 10:
                break
                
        print(f"âœ… å°ç£æ–°èæ›´æ–°å®Œæˆï¼Œå…± {len(articles)} å‰‡ç²¾è¯ã€‚")
    except Exception as e:
        print(f"âŒ å°ç£æŠ“å–éŒ¯èª¤: {e}")
    return articles

# =========================
# 3. çˆ¬å–æ—¥æœ¬æ–°è (æ—¥ç¶“æ–°è)
# =========================
# å¿½ç•¥è¨å­çš„è­¦å‘Šè¨Šæ¯
warnings.filterwarnings("ignore", category=XMLParsedAsHTMLWarning)

def get_japan_news():
    print("ğŸ” æ­£åœ¨ç²¾ç¢ºæŠ“å–æ—¥æœ¬ä¿éšªç”¢æ¥­æ–°è (ç©©å®šé™åˆ¶ç‰ˆ)...")
    # æœå°‹é—œéµå­—ï¼šç¢ºä¿ç²¾æº–å°æº–æ¥­ç•Œèˆ‡å£½éšª/æå®³ä¿éšª
    rss_url = "https://news.google.com/rss/search?q=%22%E4%BF%9D%E9%99%BA%E6%A5%AD%E7%95%8C%22%20OR%20%22%E7%94%9F%E5%91%BD%E4%BF%9D%E9%99%BA%22%20OR%20%22%E6%90%8D%E5%AE%B3%E4%BF%9D%E9%99%BA%22&hl=ja&gl=JP&ceid=JP%3Aja"
    
    articles = []
    # ç²¾ç¢ºé»‘åå–®
    JP_BLACKLIST = ["ä¿é™ºå¥—", "ä¿é™ºè¨¼"]

    try:
        response = requests.get(rss_url, headers=HEADERS, timeout=15)
        # æ—¢ç„¶æ²’æœ‰ lxmlï¼Œæˆ‘å€‘å°±çµ±ä¸€ç”¨ html.parserï¼Œä½†èª¿æ•´æŠ“å–æ¨™ç±¤çš„å¯«æ³•
        soup = BeautifulSoup(response.text, "html.parser")
        
        # åœ¨ html.parser ä¹‹ä¸‹ï¼ŒXML çš„ <item> æœƒè¢«è­˜åˆ¥ç‚º <item></item>
        items = soup.find_all("item")
        
        for item in items:
            # å˜—è©¦æŠ“å–æ¨™é¡Œèˆ‡é€£çµ
            title_tag = item.find("title")
            link_tag = item.find("link")
            
            if title_tag and link_tag:
                title = title_tag.get_text()
                # è™•ç† Google News RSS ç‰¹æœ‰çš„é€£çµè®€å–å•é¡Œ
                link = link_tag.next_sibling if link_tag.next_sibling and "http" in str(link_tag.next_sibling) else link_tag.get_text()
                link = str(link).strip()

                # éæ¿¾é‚è¼¯
                if any(word in title for word in JP_BLACKLIST):
                    continue
                
                # æ¨™é¡Œé•·åº¦æª¢æŸ¥ä¸”å¿…é ˆåŒ…å«æ ¸å¿ƒè©å½™
                if len(title) > 15 and "ä¿é™º" in title:
                    articles.append({
                        "title": title,
                        "link": link,
                        "date": TODAY_STR,
                        "source": "æ—¥æœ¬æ–°è"
                    })
            
            # ğŸ”´ å¼·åˆ¶ç…è»Šï¼šæœ€å¤šåªæ‹¿ 10 å‰‡ï¼Œçµ•å°ä¸å†å™´ 100 å‰‡
            if len(articles) >= 10:
                break
                
        print(f"âœ… æ›´æ–°å®Œæˆï¼æˆåŠŸç¯©é¸å‡º {len(articles)} å‰‡æ—¥æœ¬ç²¾è¯æ–°èã€‚")
    except Exception as e:
        print(f"âŒ æ—¥æœ¬æŠ“å–å¤±æ•—: {e}")
        
    return articles
#3.5 è«–æ–‡å®šæœŸæ›´æ–°
#
#
def get_journal_papers():
    journals = [
        "Journal of Financial Economics", "Journal of Banking and Finance", 
        "Journal of Corporate Finance", "Journal of Japanese International Economy",
        "Journal of Money, Credit and Banking", "Journal of Financial and Quantitative Analysis",
        "Review of Financial Studies","Journal of Risk and Insurance","Insurance: Mathematics and Economics"
    ]
    
    all_papers = []
    print(f"ğŸ” é–‹å§‹ç›£æ¸¬ {len(journals)} æœ¬é‡‘èé ‚åˆŠ...")

    for j in journals:
        # ä½¿ç”¨ Google News RSS é‡å°ç‰¹å®šæœŸåˆŠæœå°‹æ¨™é¡Œ
        # åŠ ä¸Š intitle: ç¢ºä¿æŠ“åˆ°çš„æ˜¯è«–æ–‡æ¨™é¡Œè€Œéæ–°èå ±å°
        rss_url = f"https://news.google.com/rss/search?q=intitle:%22{j.replace(' ', '+')}%22&hl=en-US&gl=US&ceid=US:en"
        
        try:
            response = requests.get(rss_url, headers=HEADERS, timeout=15)
            soup = BeautifulSoup(response.text, "html.parser")
            items = soup.find_all("item")

            count = 0
            for item in items:
                title_tag = item.find("title")
                link_tag = item.find("link")
                
                if title_tag and link_tag:
                    raw_title = title_tag.get_text()
                    # æŠ“é‡é»é‚è¼¯ï¼šåˆ‡é™¤æ¨™é¡Œå¾Œæ–¹çš„æœŸåˆŠåç¨± (ä¾‹å¦‚ "Title - Journal of...")
                    clean_title = raw_title.split(' - ')[0].split(' | ')[0]
                    
                    # æŠ“é€£çµé‚è¼¯
                    link = link_tag.next_sibling if link_tag.next_sibling and "http" in str(link_tag.next_sibling) else link_tag.get_text()
                    
                    all_papers.append({
                        "title": clean_title.strip(),
                        "link": str(link).strip(),
                        "journal": j,
                        "date": TODAY_STR
                    })
                    count += 1
                if count >= 3: break # æ¯å€‹æœŸåˆŠå–æœ€æ–° 3 ç¯‡ï¼Œé¿å…é é¢å¤ªé•·
            print(f"âœ… {j}: å·²æŠ“å– {count} ç¯‡")
        except Exception as e:
            print(f"âŒ {j} æŠ“å–å¤±æ•—: {e}")
            
    return all_papers
# =========================
# 4. åŸ·è¡Œèˆ‡å„²å­˜
# =========================
def main():
    tw_news = get_taiwan_news()
    jp_news = get_japan_news()
    papers = get_journal_papers()
    all_news = tw_news + jp_news
    
    # ç§»é™¤é‡è¤‡æ¨™é¡Œ
    if all_news:
        df = pd.DataFrame(all_news).drop_duplicates(subset="title")
        all_news = df.to_dict('records')
    
    if not all_news:
        all_news = [{
            "title": "ä»Šæ—¥æš«ç„¡æ›´æ–°æ–°è",
            "link": "#",
            "date": TODAY_STR,
            "source": "å°ç£æ–°è"
        }]

    # ç›´æ¥ä½¿ç”¨ json åº«å¯«å…¥ï¼Œé¿å…ä¾è³´ pandas çš„ç‰¹æ®Šæ ¼å¼
    json_path = os.path.join(OUTPUT_DIR, "news_data.json")
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(all_news, f, ensure_ascii=False, indent=4)
        
    print(f"âœ… æ›´æ–°å®Œæˆï¼æŠ“åˆ° {len(tw_news)} å‰‡å°ç£æ–°èï¼Œ{len(jp_news)} å‰‡æ—¥æœ¬æ–°èã€‚")
    # å„²å­˜è«–æ–‡è³‡æ–™
    with open(os.path.join(OUTPUT_DIR, "paper_data.json"), 'w', encoding='utf-8') as f:
        json.dump(papers, f, ensure_ascii=False, indent=4)
if __name__ == "__main__":
    main()
