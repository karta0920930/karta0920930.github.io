import requests
from bs4 import BeautifulSoup
import pandas as pd
import datetime
import os

# =========================
# 1. åŸºæœ¬è¨­å®š
# =========================
OUTPUT_DIR = "data"
if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)

TODAY_STR = datetime.datetime.today().strftime("%Y-%m-%d")
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

# =========================
# 2. çˆ¬å–å°ç£æ–°è (æ”¹ç”¨å…§å»ºè§£æå™¨)
# =========================
def get_taiwan_news():
    print("ğŸ” å˜—è©¦æŠ“å–å°ç£ä¿éšªæ–°è...")
    # ä½¿ç”¨ Google News ç¶²é ç‰ˆè€Œé RSS é¿å… XML åº«éºå¤±å•é¡Œ
    url = "https://news.google.com/search?q=%E4%BF%9D%E9%9A%AA&hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant"
    articles = []
    try:
        response = requests.get(url, headers=HEADERS, timeout=15)
        # é—œéµï¼šæ”¹ç”¨å…§å»ºçš„ html.parserï¼Œä¸éœ€è¦é¡å¤–å®‰è£ lxml
        soup = BeautifulSoup(response.text, "html.parser")
        
        # å°‹æ‰¾æ‰€æœ‰æ–°èé€£çµ
        links = soup.find_all("a", limit=50)
        for link in links:
            title = link.get_text(strip=True)
            href = link.get('href')
            
            # éæ¿¾æ¨™é¡Œé•·åº¦ï¼Œç¢ºä¿å®ƒæ˜¯æ–°èæ¨™é¡Œè€ŒéæŒ‰éˆ•æ–‡å­—
            if href and href.startswith('./articles/') and len(title) > 10:
                full_link = "https://news.google.com" + href[1:]
                articles.append({
                    "title": title,
                    "link": full_link,
                    "date": TODAY_STR,
                    "source": "å°ç£æ–°è"
                })
                if len(articles) >= 15: break # æŠ“ 15 å‰‡å°±å¤ 
    except Exception as e:
        print(f"âŒ å°ç£æŠ“å–éŒ¯èª¤: {e}")
    return articles

# =========================
# 3. çˆ¬å–æ—¥æœ¬æ–°è (æ”¾å¯¬è¦å‰‡ç‰ˆ)
# =========================
def get_japan_news():
    print("ğŸ” æŠ“å–æ—¥æœ¬æ–°è...")
    url = "https://www.nikkei.com/search?keyword=%E4%BF%9D%E9%99%BA"
    articles = []
    try:
        response = requests.get(url, headers=HEADERS, timeout=15)
        soup = BeautifulSoup(response.text, "html.parser")
        
        # æ”¾å¯¬æ¢ä»¶ï¼šæŠ“å–æ‰€æœ‰åŒ…å«ã€Œä¿éšªã€é—œéµå­—çš„é€£çµ
        links = soup.find_all("a")
        for link in links:
            title = link.get_text(strip=True)
            href = link.get('href')
            
            if href and "ä¿é™º" in title and len(title) > 15:
                full_link = href
                if href.startswith('/'):
                    full_link = "https://www.nikkei.com" + href
                
                articles.append({
                    "title": title,
                    "link": full_link,
                    "date": TODAY_STR,
                    "source": "æ—¥æœ¬æ–°è"
                })
                if len(articles) >= 25: break
    except Exception as e:
        print(f"âŒ æ—¥æœ¬æŠ“å–éŒ¯èª¤: {e}")
    return articles

# =========================
# 4. åŸ·è¡Œèˆ‡å„²å­˜
# =========================
def main():
    tw_news = get_taiwan_news()
    jp_news = get_japan_news()
    
    # ç§»é™¤é‡è¤‡æ¨™é¡Œ
    all_news = tw_news + jp_news
    if all_news:
        df = pd.DataFrame(all_news).drop_duplicates(subset="title")
        all_news = df.to_dict('records')
    
    if not all_news:
        print("âš ï¸ å®Œå…¨æ²’æŠ“åˆ°æ–°èï¼Œç”¢ç”Ÿä¸€æ¢æ¸¬è©¦è³‡æ–™")
        all_news = [{
            "title": "ä»Šæ—¥æš«ç„¡æ›´æ–°æ–°è (ç³»çµ±è‡ªå‹•åµæ¸¬ä¸­)",
            "link": "#",
            "date": TODAY_STR,
            "source": "å°ç£æ–°è"
        }]

    # è¼¸å‡º JSON
    import json
    json_path = os.path.join(OUTPUT_DIR, "news_data.json")
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(all_news, f, ensure_ascii=False, indent=4)
        
    print(f"âœ… æ›´æ–°å®Œæˆï¼å…± {len(all_news)} å‰‡æ–°èå·²å¯«å…¥ {json_path}")

if __name__ == "__main__":
    main()
