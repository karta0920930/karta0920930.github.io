import requests
from bs4 import BeautifulSoup
import pandas as pd
import datetime
import os
import json
import warnings
from bs4 import BeautifulSoup, XMLParsedAsHTMLWarning
# =========================
# 1. åŸºæœ¬è¨­å®š
# =========================
OUTPUT_DIR = "data"
if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)

TODAY_STR = datetime.datetime.today().strftime("%Y-%m-%d")
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

# =========================
# 2. çˆ¬å–å°ç£æ–°è (æ”¹ç”¨ Yahoo æ–°è - ä¿éšªé—œéµå­—)
# =========================
def get_taiwan_news():
    print("ğŸ” å˜—è©¦æŠ“å–å°ç£ä¿éšªæ–°è (Yahoo)...")
    # æŠ“å– Yahoo æœå°‹ã€Œä¿éšªã€çš„æœ€æ–°æ–°èçµæœ
    url = "https://tw.news.yahoo.com/search?p=%E4%BF%9D%E9%9A%AA&fr=news"
    articles = []
    try:
        response = requests.get(url, headers=HEADERS, timeout=15)
        soup = BeautifulSoup(response.text, "html.parser")
        
        # Yahoo æ–°èæ¨™é¡Œé€šå¸¸åœ¨ <h3> ä¸­
        items = soup.find_all("h3")
        for item in items:
            title_tag = item.find("a")
            if title_tag:
                title = title_tag.get_text(strip=True)
                href = title_tag.get('href')
                
                # ç¢ºä¿é€£çµå®Œæ•´ä¸”æ¨™é¡Œé•·åº¦åˆç†
                if href and len(title) > 8:
                    if not href.startswith('http'):
                        href = "https://tw.news.yahoo.com" + href
                    
                    articles.append({
                        "title": title,
                        "link": href,
                        "date": TODAY_STR,
                        "source": "å°ç£æ–°è"
                    })
            if len(articles) >= 15: break
    except Exception as e:
        print(f"âŒ å°ç£æŠ“å–éŒ¯èª¤: {e}")
    return articles

# =========================
# 3. çˆ¬å–æ—¥æœ¬æ–°è (æ—¥ç¶“æ–°è)
# =========================
# å¿½ç•¥è¨å­çš„è­¦å‘Šè¨Šæ¯
warnings.filterwarnings("ignore", category=XMLParsedAsHTMLWarning)

def get_japan_news():
    print("ğŸ” æ­£åœ¨ç²¾ç¢ºæŠ“å–æ—¥æœ¬ä¿éšªç”¢æ¥­æ–°è (ç©©å®šé™åˆ¶ç‰ˆ)...")
    # æœå°‹é—œéµå­—ï¼šç¢ºä¿ç²¾æº–å°æº–æ¥­ç•Œèˆ‡å£½éšª/æå®³ä¿éšª
    rss_url = "https://news.google.com/rss/search?q=%22%E4%BF%9D%E9%99%BA%E6%A5%AD%E7%95%8C%22%20OR%20%22%E7%94%9F%E5%91%BD%E4%BF%9D%E9%99%BA%22%20OR%20%22%E6%90%8D%E5%AE%B3%E4%BF%9D%E9%99%BA%22&hl=ja&gl=JP&ceid=JP%3Aja"
    
    articles = []
    # ç²¾ç¢ºé»‘åå–®
    JP_BLACKLIST = ["ä¿é™ºå¥—", "ç¤¾ä¼šä¿é™º", "é›‡ç”¨ä¿é™º", "å¥åº·ä¿é™º", "ä¿é™ºæ–™æ§é™¤"]

    try:
        response = requests.get(rss_url, headers=HEADERS, timeout=15)
        # æ—¢ç„¶æ²’æœ‰ lxmlï¼Œæˆ‘å€‘å°±çµ±ä¸€ç”¨ html.parserï¼Œä½†èª¿æ•´æŠ“å–æ¨™ç±¤çš„å¯«æ³•
        soup = BeautifulSoup(response.text, "html.parser")
        
        # åœ¨ html.parser ä¹‹ä¸‹ï¼ŒXML çš„ <item> æœƒè¢«è­˜åˆ¥ç‚º <item></item>
        items = soup.find_all("item")
        
        for item in items:
            # å˜—è©¦æŠ“å–æ¨™é¡Œèˆ‡é€£çµ
            title_tag = item.find("title")
            link_tag = item.find("link")
            
            if title_tag and link_tag:
                title = title_tag.get_text()
                # è™•ç† Google News RSS ç‰¹æœ‰çš„é€£çµè®€å–å•é¡Œ
                link = link_tag.next_sibling if link_tag.next_sibling and "http" in str(link_tag.next_sibling) else link_tag.get_text()
                link = str(link).strip()

                # éæ¿¾é‚è¼¯
                if any(word in title for word in JP_BLACKLIST):
                    continue
                
                # æ¨™é¡Œé•·åº¦æª¢æŸ¥ä¸”å¿…é ˆåŒ…å«æ ¸å¿ƒè©å½™
                if len(title) > 15 and "ä¿é™º" in title:
                    articles.append({
                        "title": title,
                        "link": link,
                        "date": TODAY_STR,
                        "source": "æ—¥æœ¬æ–°è"
                    })
            
            # ğŸ”´ å¼·åˆ¶ç…è»Šï¼šæœ€å¤šåªæ‹¿ 10 å‰‡ï¼Œçµ•å°ä¸å†å™´ 100 å‰‡
            if len(articles) >= 10:
                break
                
        print(f"âœ… æ›´æ–°å®Œæˆï¼æˆåŠŸç¯©é¸å‡º {len(articles)} å‰‡æ—¥æœ¬ç²¾è¯æ–°èã€‚")
    except Exception as e:
        print(f"âŒ æ—¥æœ¬æŠ“å–å¤±æ•—: {e}")
        
    return articles
#3.5 è«–æ–‡å®šæœŸæ›´æ–°
#
#
def get_journal_papers():
    print("ğŸ” æ­£åœ¨æª¢ç´¢ Journal of Risk and Insurance æœ€æ–°è«–æ–‡...")
    # ä½¿ç”¨ Google News RSS æœå°‹å­¸è¡“æœŸåˆŠæ›´æ–°ï¼Œé€™å° GitHub Actions æœ€ç©©å®š
    rss_url = "https://news.google.com/rss/search?q=source:%22Journal+of+Risk+and+Insurance%22&hl=en-US&gl=US&ceid=US:en"
    papers = []
    try:
        response = requests.get(rss_url, headers=HEADERS, timeout=15)
        soup = BeautifulSoup(response.content, "xml")
        items = soup.find_all("item")

        for item in items:
            papers.append({
                "title": item.title.text,
                "link": item.link.text,
                "date": item.pubDate.text if item.pubDate else TODAY_STR,
                "journal": "JRI"
            })
            if len(papers) >= 10: break
        print(f"âœ… æˆåŠŸæŠ“å– {len(papers)} ç¯‡æœ€æ–°è«–æ–‡")
    except Exception as e:
        print(f"âŒ è«–æ–‡æŠ“å–å¤±æ•—: {e}")
    return papers
# =========================
# 4. åŸ·è¡Œèˆ‡å„²å­˜
# =========================
def main():
    tw_news = get_taiwan_news()
    jp_news = get_japan_news()
    papers = get_journal_papers()
    all_news = tw_news + jp_news
    
    # ç§»é™¤é‡è¤‡æ¨™é¡Œ
    if all_news:
        df = pd.DataFrame(all_news).drop_duplicates(subset="title")
        all_news = df.to_dict('records')
    
    if not all_news:
        all_news = [{
            "title": "ä»Šæ—¥æš«ç„¡æ›´æ–°æ–°è",
            "link": "#",
            "date": TODAY_STR,
            "source": "å°ç£æ–°è"
        }]

    # ç›´æ¥ä½¿ç”¨ json åº«å¯«å…¥ï¼Œé¿å…ä¾è³´ pandas çš„ç‰¹æ®Šæ ¼å¼
    json_path = os.path.join(OUTPUT_DIR, "news_data.json")
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(all_news, f, ensure_ascii=False, indent=4)
        
    print(f"âœ… æ›´æ–°å®Œæˆï¼æŠ“åˆ° {len(tw_news)} å‰‡å°ç£æ–°èï¼Œ{len(jp_news)} å‰‡æ—¥æœ¬æ–°èã€‚")
    # å„²å­˜è«–æ–‡è³‡æ–™
    with open(os.path.join(OUTPUT_DIR, "paper_data.json"), 'w', encoding='utf-8') as f:
        json.dump(papers, f, ensure_ascii=False, indent=4)
if __name__ == "__main__":
    main()
