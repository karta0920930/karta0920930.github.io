import requests
from bs4 import BeautifulSoup
import pandas as pd
import datetime
import os

# =========================
# 1. åŸºæœ¬è¨­å®š
# =========================
OUTPUT_DIR = "data"
if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)

TODAY_STR = datetime.datetime.today().strftime("%Y-%m-%d")
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

# =========================
# 2. çˆ¬å–å°ç£æ–°è (ä½¿ç”¨ç©©å®š RSS ä¾†æº)
# =========================
def get_taiwan_news():
    print("ğŸ” å˜—è©¦å¾ RSS æŠ“å–å°ç£ä¿éšªæ–°è...")
    # ä½¿ç”¨ Google News RSS æ ¼å¼ï¼Œé€™åœ¨ GitHub Actions ä¸Šæ¥µåº¦ç©©å®š
    rss_url = "https://news.google.com/rss/search?q=%E4%BF%9D%E9%9A%AA&hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant"
    articles = []
    try:
        response = requests.get(rss_url, headers=HEADERS, timeout=15)
        soup = BeautifulSoup(response.content, "xml") # RSS è¦ç”¨ xml è§£æ
        items = soup.find_all("item")
        
        for item in items[:15]: # æŠ“å‰ 15 å‰‡
            articles.append({
                "title": item.title.text,
                "link": item.link.text,
                "date": TODAY_STR,
                "source": "å°ç£æ–°è"
            })
    except Exception as e:
        print(f"âŒ å°ç£ RSS æŠ“å–éŒ¯èª¤: {e}")
    return articles

# =========================
# 3. çˆ¬å–æ—¥æœ¬æ–°è (æ—¥ç¶“æ–°è)
# =========================
def get_japan_news():
    print("ğŸ” æŠ“å–æ—¥æœ¬æ–°è...")
    url = "https://www.nikkei.com/search?keyword=%E4%BF%9D%E9%99%BA"
    articles = []
    try:
        response = requests.get(url, headers=HEADERS, timeout=15)
        soup = BeautifulSoup(response.text, "html.parser")
        # æ“´å¤§æœç´¢ç¯„åœï¼Œé¿å… Selector å¤±æ•ˆ
        items = soup.find_all("article")
        
        for item in items:
            title_tag = item.find("h3") or item.find("a")
            link_tag = item.find("a")
            if title_tag and link_tag:
                title = title_tag.get_text(strip=True)
                link = link_tag.get('href')
                if link.startswith('/'):
                    link = "https://www.nikkei.com" + link
                
                # éæ¿¾å¤ªçŸ­çš„æ¨™é¡Œï¼ˆé€šå¸¸æ˜¯å°è¦½æ¨™ç±¤ï¼‰
                if len(title) > 10:
                    articles.append({
                        "title": title,
                        "link": link,
                        "date": TODAY_STR,
                        "source": "æ—¥æœ¬æ–°è"
                    })
    except Exception as e:
        print(f"âŒ æ—¥æœ¬æŠ“å–éŒ¯èª¤: {e}")
    return articles

# =========================
# 4. åŸ·è¡Œèˆ‡å„²å­˜
# =========================
def main():
    tw_news = get_taiwan_news()
    jp_news = get_japan_news()
    
    all_news = tw_news + jp_news
    
    if not all_news:
        print("âš ï¸ å®Œå…¨æ²’æŠ“åˆ°æ–°èï¼Œç”¢ç”Ÿä¸€æ¢æ¸¬è©¦è³‡æ–™é¿å…ç¨‹å¼ä¸­æ–·")
        all_news = [{
            "title": "ç³»çµ±æ¸¬è©¦ï¼šç›®å‰ç·šä¸Šæš«ç„¡æœ€æ–°æ–°èï¼Œè«‹ç¨å¾Œå†è©¦",
            "link": "#",
            "date": TODAY_STR,
            "source": "å°ç£æ–°è"
        }]

    df = pd.DataFrame(all_news)
    
    # å­˜æˆ JSON
    json_path = os.path.join(OUTPUT_DIR, "news_data.json")
    df.to_json(json_path, orient="records", force_ascii=False, indent=4)
    print(f"âœ… æ›´æ–°å®Œæˆï¼å…± {len(df)} å‰‡æ–°èå·²å¯«å…¥ {json_path}")

if __name__ == "__main__":
    main()
