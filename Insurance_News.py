{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27135b1d-d579-43cf-a2b5-84ba590098fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<string>, line 56)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mFile \u001b[39m\u001b[32m<string>:56\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mreturn pd.DataFrame(articles)\u001b[39m\n                                 ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from wordcloud import WordCloud\n",
    "import os\n",
    "import matplotlib\n",
    "\n",
    "# --- ä¿®æ”¹ 1: è·¯å¾‘è¨­å®šæ”¹ç‚ºç›¸å°è·¯å¾‘ ---\n",
    "# é€™æ¨£åœ¨ GitHub ç’°å¢ƒä¸­æœƒç›´æ¥ç”¢ç”Ÿåœ¨å°ˆæ¡ˆæ ¹ç›®éŒ„\n",
    "OUTPUT_DIR = \"data\" \n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "KEYWORD = \"ä¿é™º\"\n",
    "TODAY = datetime.datetime.today().strftime(\"%Y-%m-%d\") # å»ºè­°ç”¨æ©«ç·šï¼Œç¶²é é¡¯ç¤ºè¼ƒç¾è§€\n",
    "\n",
    "categories = {\n",
    "    \"å†ä¿é™º\": [\"å†ä¿é™º\", \"reinsurance\"],\n",
    "    \"ESG\": [\"ESG\", \"ã‚µã‚¹ãƒ†ãƒŠ\", \"æ°—å€™å¤‰å‹•\", \"è„±ç‚­ç´ \"],\n",
    "    \"ç½å®³ãƒªã‚¹ã‚¯\": [\"åœ°éœ‡\", \"å°é¢¨\", \"æ´ªæ°´\", \"è±ªé›¨\"],\n",
    "    \"ç”Ÿå‘½ä¿é™º\": [\"ç”Ÿå‘½ä¿é™º\", \"ç”Ÿä¿\"],\n",
    "    \"æå®³ä¿é™º\": [\"æä¿\", \"ç«ç½ä¿é™º\", \"è‡ªå‹•è»Šä¿é™º\"]\n",
    "}\n",
    "\n",
    "def get_nikkei_news():\n",
    "    url = f\"https://www.nikkei.com/search?keyword={KEYWORD}\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"}\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        articles = []\n",
    "\n",
    "        # é€™è£¡æ ¹æ“šæ—¥ç¶“ç›®å‰çš„çµæ§‹å¾®èª¿æŠ“å–é‚è¼¯\n",
    "        for a in soup.find_all(\"a\"):\n",
    "            title = a.text.strip()\n",
    "            link = a.get(\"href\")\n",
    "            \n",
    "            if link and link.startswith(\"/\"):\n",
    "                link = \"https://www.nikkei.com\" + link\n",
    "                \n",
    "            if KEYWORD in title and len(title) > 5:\n",
    "                articles.append({\n",
    "                    \"title\": title,\n",
    "                    \"link\": link,\n",
    "                    \"date\": TODAY\n",
    "                })\n",
    "        return pd.DataFrame(articles)\n",
    "    except Exception as e:\n",
    "        print(f\"æŠ“å–å¤±æ•—: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def categorize(title):\n",
    "    for category, keywords in categories.items():\n",
    "        for word in keywords:\n",
    "            if word in title:\n",
    "                return category\n",
    "    return \"ãã®ä»–\"\n",
    "\n",
    "def main():\n",
    "    print(\"ğŸ” æŠ“å–æ–°èä¸­...\")\n",
    "    df = get_nikkei_news()\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"âŒ æ²’æœ‰æŠ“åˆ°è³‡æ–™\")\n",
    "        return\n",
    "\n",
    "    df.drop_duplicates(subset=\"title\", inplace=True)\n",
    "    df[\"category\"] = df[\"title\"].apply(categorize)\n",
    "\n",
    "    # --- ä¿®æ”¹ 2: åœ–è¡¨ç”ŸæˆåŠ å…¥ä¾‹å¤–è™•ç† ---\n",
    "    # é€™æ˜¯ç‚ºäº†é˜²æ­¢ GitHub Actions å› ç‚ºæ‰¾ä¸åˆ°å­—é«”è€Œä¸­æ–·\n",
    "    try:\n",
    "        # å˜—è©¦è¨­å®šæ—¥æ–‡å­—é«”ï¼Œå¦‚æœå¤±æ•—å°±ç”¨é è¨­å€¼\n",
    "        matplotlib.rcParams['font.family'] = 'DejaVu Sans' \n",
    "        category_count = df[\"category\"].value_counts()\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        category_count.plot(kind=\"bar\")\n",
    "        plt.title(\"News Distribution\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUTPUT_DIR, f\"category_distribution.png\"))\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ åœ–è¡¨ç”Ÿæˆè·³é: {e}\")\n",
    "\n",
    "    # --- JSON è¼¸å‡º (é€™æ˜¯ç¶²é é‹ä½œçš„æ ¸å¿ƒï¼Œä¸èƒ½å‡ºéŒ¯) ---\n",
    "    json_output = os.path.join(OUTPUT_DIR, \"news_data.json\")\n",
    "    df.to_json(json_output, orient=\"records\", force_ascii=False, indent=4)\n",
    "    \n",
    "    # åŒæ™‚ä¿ç•™ Excel å‚™ä»½\n",
    "    output_file = os.path.join(OUTPUT_DIR, f\"insurance_news_{TODAY}.xlsx\")\n",
    "    df.to_excel(output_file, index=False)\n",
    "\n",
    "    print(f\"âœ… åˆ†æå®Œæˆï¼è³‡æ–™å·²å­˜è‡³ {json_output}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63c1126-2e35-4871-af16-0d996d139051",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.font_manager as fm\n",
    "for font in fm.findSystemFonts():\n",
    "    print(font)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
