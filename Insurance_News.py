import requests
from bs4 import BeautifulSoup
import pandas as pd
import datetime
import os
import json
import warnings
from bs4 import BeautifulSoup, XMLParsedAsHTMLWarning
# =========================
# 1. åŸºæœ¬è¨­å®š
# =========================
OUTPUT_DIR = "data"
if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)

TODAY_STR = datetime.datetime.today().strftime("%Y-%m-%d")
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

# =========================
# 2. çˆ¬å–å°ç£æ–°è (æ”¹ç”¨ Yahoo æ–°è - ä¿éšªé—œéµå­—)
# =========================
def get_taiwan_news():
    print("ğŸ” å˜—è©¦æŠ“å–å°ç£ä¿éšªæ–°è (Yahoo)...")
    # æŠ“å– Yahoo æœå°‹ã€Œä¿éšªã€çš„æœ€æ–°æ–°èçµæœ
    url = "https://tw.news.yahoo.com/search?p=%E4%BF%9D%E9%9A%AA&fr=news"
    articles = []
    TW_BLACKLIST = ["ä¿éšªå¥—",  "å»£å‘Š", "ç™¼è²¡"]
    try:
        response = requests.get(url, headers=HEADERS, timeout=15)
        soup = BeautifulSoup(response.text, "html.parser")
        
        # Yahoo æ–°èæ¨™é¡Œé€šå¸¸åœ¨ <h3> ä¸­
        items = soup.find_all("h3")
        for item in items:
            title_tag = item.find("a")
            if title_tag:
                raw_title = title_tag.get_text(strip=True)
                href = title_tag.get('href')
                
                # --- æŠ“é‡é»é‚è¼¯ 1ï¼šç§»é™¤ä¾†æºå­—æ¨£ ---
                # å¾ˆå¤šæ¨™é¡Œæœƒé•·é€™æ¨£ï¼šã€Œä¿éšªæ¥­ç²åˆ©å‰µæ–°é«˜ - Yahooå¥‡æ‘©æ–°èã€
                # æˆ‘å€‘åªå– '-' æˆ–æ˜¯ '|' ä¹‹å‰çš„å…§å®¹
                clean_title = raw_title.split(' - ')[0].split(' | ')[0].split(' (')[0]

                # --- æŠ“é‡é»é‚è¼¯ 2ï¼šéæ¿¾ ---
                if any(word in clean_title for word in TW_BLACKLIST):
                    continue

                if href and len(clean_title) > 10:
                    if not href.startswith('http'):
                        href = "https://tw.news.yahoo.com" + href
                    
                    articles.append({
                        "title": clean_title, # ä½¿ç”¨æ¸…ç†å¾Œçš„é‡é»æ¨™é¡Œ
                        "link": href,
                        "date": TODAY_STR,
                        "source": "å°ç£æ–°è"
                    })
            
            # --- æ•¸é‡é™åˆ¶ï¼šé™ 10 å‰‡ ---
            if len(articles) >= 10:
                break
                
        print(f"âœ… å°ç£æ–°èæ›´æ–°å®Œæˆï¼Œå…± {len(articles)} å‰‡ç²¾è¯ã€‚")
    except Exception as e:
        print(f"âŒ å°ç£æŠ“å–éŒ¯èª¤: {e}")
    return articles

# =========================
# 3. çˆ¬å–æ—¥æœ¬æ–°è (æ—¥ç¶“æ–°è)
# =========================
# å¿½ç•¥è¨å­çš„è­¦å‘Šè¨Šæ¯
warnings.filterwarnings("ignore", category=XMLParsedAsHTMLWarning)

def get_japan_news():
    print("ğŸ” æ­£åœ¨ç²¾ç¢ºæŠ“å–æ—¥æœ¬ä¿éšªç”¢æ¥­æ–°è (ç©©å®šé™åˆ¶ç‰ˆ)...")
    # æœå°‹é—œéµå­—ï¼šç¢ºä¿ç²¾æº–å°æº–æ¥­ç•Œèˆ‡å£½éšª/æå®³ä¿éšª
    rss_url = "https://news.google.com/rss/search?q=%22%E4%BF%9D%E9%99%BA%E6%A5%AD%E7%95%8C%22%20OR%20%22%E7%94%9F%E5%91%BD%E4%BF%9D%E9%99%BA%22%20OR%20%22%E6%90%8D%E5%AE%B3%E4%BF%9D%E9%99%BA%22&hl=ja&gl=JP&ceid=JP%3Aja"
    
    articles = []
    # ç²¾ç¢ºé»‘åå–®
    JP_BLACKLIST = ["ä¿é™ºå¥—", "ä¿é™ºè¨¼"]

    try:
        response = requests.get(rss_url, headers=HEADERS, timeout=15)
        # æ—¢ç„¶æ²’æœ‰ lxmlï¼Œæˆ‘å€‘å°±çµ±ä¸€ç”¨ html.parserï¼Œä½†èª¿æ•´æŠ“å–æ¨™ç±¤çš„å¯«æ³•
        soup = BeautifulSoup(response.text, "html.parser")
        
        # åœ¨ html.parser ä¹‹ä¸‹ï¼ŒXML çš„ <item> æœƒè¢«è­˜åˆ¥ç‚º <item></item>
        items = soup.find_all("item")
        
        for item in items:
            # å˜—è©¦æŠ“å–æ¨™é¡Œèˆ‡é€£çµ
            title_tag = item.find("title")
            link_tag = item.find("link")
            
            if title_tag and link_tag:
                title = title_tag.get_text()
                # è™•ç† Google News RSS ç‰¹æœ‰çš„é€£çµè®€å–å•é¡Œ
                link = link_tag.next_sibling if link_tag.next_sibling and "http" in str(link_tag.next_sibling) else link_tag.get_text()
                link = str(link).strip()

                # éæ¿¾é‚è¼¯
                if any(word in title for word in JP_BLACKLIST):
                    continue
                
                # æ¨™é¡Œé•·åº¦æª¢æŸ¥ä¸”å¿…é ˆåŒ…å«æ ¸å¿ƒè©å½™
                if len(title) > 15 and "ä¿é™º" in title:
                    articles.append({
                        "title": title,
                        "link": link,
                        "date": TODAY_STR,
                        "source": "æ—¥æœ¬æ–°è"
                    })
            
            # ğŸ”´ å¼·åˆ¶ç…è»Šï¼šæœ€å¤šåªæ‹¿ 10 å‰‡ï¼Œçµ•å°ä¸å†å™´ 100 å‰‡
            if len(articles) >= 10:
                break
                
        print(f"âœ… æ›´æ–°å®Œæˆï¼æˆåŠŸç¯©é¸å‡º {len(articles)} å‰‡æ—¥æœ¬ç²¾è¯æ–°èã€‚")
    except Exception as e:
        print(f"âŒ æ—¥æœ¬æŠ“å–å¤±æ•—: {e}")
        
    return articles
#3.5 è«–æ–‡å®šæœŸæ›´æ–°
#
#
def get_journal_papers():
    journals = [
        "Journal of Financial Economics", "Journal of Banking and Finance", 
        "Journal of Corporate Finance", "Journal of Japanese International Economy",
        "Journal of Money, Credit and Banking", "Journal of Financial and Quantitative Analysis",
        "Review of Financial Studies", "Journal of Risk and Insurance", "Insurance: Mathematics and Economics"
    ]
    
    all_papers = []
    print(f"ğŸ” é–‹å§‹ç›£æ¸¬ {len(journals)} æœ¬é‡‘èé ‚åˆŠ...")

    for j in journals:
        rss_url = f"https://news.google.com/rss/search?q=intitle:%22{j.replace(' ', '+')}%22&hl=en-US&gl=US&ceid=US:en"
        
        try:
            response = requests.get(rss_url, headers=HEADERS, timeout=15)
            soup = BeautifulSoup(response.text, "html.parser")
            items = soup.find_all("item")

            count = 0
            for item in items:
                title_tag = item.find("title")
                link_tag = item.find("link")
                
                if title_tag and link_tag:
                    raw_title = title_tag.get_text()
                    clean_title = raw_title.split(' - ')[0].split(' | ')[0]
                    
                    # --- ç¸®æ’ä¿®æ­£å€é–‹å§‹ ---
                    raw_link = link_tag.get_text()
                    if not raw_link or "http" not in raw_link:
                        if link_tag.next_sibling:
                            raw_link = str(link_tag.next_sibling).strip()
                        else:
                            raw_link = ""

                    if "http" in raw_link:
                        all_papers.append({
                            "title": clean_title.strip(),
                            "link": raw_link,
                            "journal": j,
                            "date": TODAY_STR
                        })
                        count += 1
                    # --- ç¸®æ’ä¿®æ­£å€çµæŸ ---

                if count >= 3: 
                    break 
            print(f"âœ… {j}: å·²æŠ“å– {count} ç¯‡")
        except Exception as e:
            print(f"âŒ {j} æŠ“å–å¤±æ•—: {e}")
            
    return all_papers
# =========================
# 4. åŸ·è¡Œèˆ‡å„²å­˜
# =========================
def main():
    # 1. ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)

    # 2. åŸ·è¡ŒæŠ“å–
    tw_news = get_taiwan_news()
    jp_news = get_japan_news()
    papers_list = get_journal_papers()  # æ”¹åé¿å…æ··æ·†
    
    # --- è™•ç†æ–°èè³‡æ–™ ---
    all_news = tw_news + jp_news
    if all_news:
        df_news = pd.DataFrame(all_news).drop_duplicates(subset="title")
        all_news = df_news.to_dict('records')
    else:
        all_news = [{
            "title": "ä»Šæ—¥æš«ç„¡æ›´æ–°æ–°è",
            "link": "#",
            "date": TODAY_STR,
            "source": "ç³»çµ±è¨Šæ¯"
        }]

    # å¯«å…¥æ–°è JSON
    news_json_path = os.path.join(OUTPUT_DIR, "news_data.json")
    with open(news_json_path, 'w', encoding='utf-8') as f:
        json.dump(all_news, f, ensure_ascii=False, indent=4)
        
    print(f"âœ… æ–°èæ›´æ–°å®Œæˆï¼å­˜å…¥ {len(all_news)} å‰‡ (å°:{len(tw_news)}/æ—¥:{len(jp_news)})")

    # --- è™•ç†è«–æ–‡è³‡æ–™ ---
    if papers_list:
        # è«–æ–‡ä¹Ÿåšå»é‡è™•ç†
        df_papers = pd.DataFrame(papers_list).drop_duplicates(subset="title")
        papers_final = df_papers.to_dict('records')
    else:
        papers_final = [] # å¦‚æœæ²’æŠ“åˆ°å°±ç•™ç©ºåˆ—è¡¨

    # å¯«å…¥è«–æ–‡ JSON
    paper_json_path = os.path.join(OUTPUT_DIR, "paper_data.json")
    with open(paper_json_path, 'w', encoding='utf-8') as f:
        json.dump(papers_final, f, ensure_ascii=False, indent=4)
        
    print(f"âœ… è«–æ–‡æ›´æ–°å®Œæˆï¼å…±å­˜å…¥ {len(papers_final)} ç¯‡è«–æ–‡ã€‚")

if __name__ == "__main__":
    main()
