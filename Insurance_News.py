import requests
from bs4 import BeautifulSoup
import pandas as pd
import datetime
import os
import json
import warnings
from bs4 import BeautifulSoup, XMLParsedAsHTMLWarning
# =========================
# 1. åŸºæœ¬è¨­å®š
# =========================
OUTPUT_DIR = "data"
if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)

TODAY_STR = datetime.datetime.today().strftime("%Y-%m-%d")
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

# =========================
# 2. çˆ¬å–å°ç£æ–°è (æ”¹ç”¨ Yahoo æ–°è - ä¿éšªé—œéµå­—)
# =========================
def get_taiwan_news():
    print("ğŸ” å˜—è©¦æŠ“å–å°ç£ä¿éšªæ–°è (Yahoo)...")
    # æŠ“å– Yahoo æœå°‹ã€Œä¿éšªã€çš„æœ€æ–°æ–°èçµæœ
    url = "https://tw.news.yahoo.com/search?p=%E4%BF%9D%E9%9A%AA&fr=news"
    articles = []
    TW_BLACKLIST = ["ä¿éšªå¥—",  "å»£å‘Š", "ç™¼è²¡"]
    try:
        response = requests.get(url, headers=HEADERS, timeout=15)
        soup = BeautifulSoup(response.text, "html.parser")
        
        # Yahoo æ–°èæ¨™é¡Œé€šå¸¸åœ¨ <h3> ä¸­
        items = soup.find_all("h3")
        for item in items:
            title_tag = item.find("a")
            if title_tag:
                raw_title = title_tag.get_text(strip=True)
                href = title_tag.get('href')
                
                # --- æŠ“é‡é»é‚è¼¯ 1ï¼šç§»é™¤ä¾†æºå­—æ¨£ ---
                # å¾ˆå¤šæ¨™é¡Œæœƒé•·é€™æ¨£ï¼šã€Œä¿éšªæ¥­ç²åˆ©å‰µæ–°é«˜ - Yahooå¥‡æ‘©æ–°èã€
                # æˆ‘å€‘åªå– '-' æˆ–æ˜¯ '|' ä¹‹å‰çš„å…§å®¹
                clean_title = raw_title.split(' - ')[0].split(' | ')[0].split(' (')[0]

                # --- æŠ“é‡é»é‚è¼¯ 2ï¼šéæ¿¾ ---
                if any(word in clean_title for word in TW_BLACKLIST):
                    continue

                if href and len(clean_title) > 10:
                    if not href.startswith('http'):
                        href = "https://tw.news.yahoo.com" + href
                    
                    articles.append({
                        "title": clean_title, # ä½¿ç”¨æ¸…ç†å¾Œçš„é‡é»æ¨™é¡Œ
                        "link": href,
                        "date": TODAY_STR,
                        "source": "å°ç£æ–°è"
                    })
            
            # --- æ•¸é‡é™åˆ¶ï¼šé™ 10 å‰‡ ---
            if len(articles) >= 10:
                break
                
        print(f"âœ… å°ç£æ–°èæ›´æ–°å®Œæˆï¼Œå…± {len(articles)} å‰‡ç²¾è¯ã€‚")
    except Exception as e:
        print(f"âŒ å°ç£æŠ“å–éŒ¯èª¤: {e}")
    return articles

# =========================
# 3. çˆ¬å–æ—¥æœ¬æ–°è (æ—¥ç¶“æ–°è)
# =========================
# å¿½ç•¥è¨å­çš„è­¦å‘Šè¨Šæ¯
warnings.filterwarnings("ignore", category=XMLParsedAsHTMLWarning)

def get_professional_insurance_news():
    print("ğŸ” æ­£åœ¨éæ¿¾ä¿éšªæ¥­æ·±åº¦ç ”ç©¶èˆ‡å°ˆæ¥­æ–°è...")
    
    # ä½¿ç”¨ OR é‚è¼¯çµ„åˆå°ˆæ¥­é—œéµå­—
    # åŒ…å«ï¼šä¿éšªæ¥­ç•Œé¢ã€èª¿æŸ¥å ±å‘Šã€ä¸­æœŸè¨ˆç•«ã€é‡‘èå»³å‹•æ…‹
    keywords = [
        '"ä¿é™ºæ¥­ç•Œ"', 
        '"ãƒ•ã‚¡ã‚¯ãƒˆãƒ–ãƒƒã‚¯"', 
        '"æ„è­˜èª¿æŸ»"', 
        '"ä¸­æœŸçµŒå–¶è¨ˆç”»"', 
        '"æ–°å•†å“ç™ºè¡¨"',
        '"é‡‘èåº ç›£ç£"'
    ]
    query = " OR ".join(keywords)
    rss_url = f"https://news.google.com/rss/search?q={query}&hl=ja&gl=JP&ceid=JP%3Aja"
    
    articles = []
    # å°ˆæ¥­é»‘åå–®ï¼šéæ¿¾æ‰ä¸€èˆ¬çš„ç¤¾æœƒæ¡ˆä»¶æˆ–å»£å‘Š
    PROFESSIONAL_BLACKLIST = ["é€®æ•", "ç«ç½", "äº‹æ•…ç¾å ´", "ä¿é™ºé‡‘è©æ¬º"]

    try:
        response = requests.get(rss_url, headers=HEADERS, timeout=15)
        soup = BeautifulSoup(response.content, "xml")
        items = soup.find_all("item")
        
        for item in items:
            title = item.title.text
            link = item.link.text
            
            # é‚è¼¯éæ¿¾ï¼š
            # 1. æ’é™¤é»‘åå–®
            if any(word in title for word in PROFESSIONAL_BLACKLIST):
                continue
                
            # 2. å¼·åŒ–éæ¿¾ï¼šå¿…é ˆåŒ…å«ä»¥ä¸‹ã€Œå¾æ¥­äººå“¡ã€æ„Ÿèˆˆè¶£çš„è©
            pro_filters = ["ç™ºè¡Œ", "èª¿æŸ»", "ç™ºè¡¨", "é–‹å§‹", "å°å…¥", "DX", "æˆ¦ç•¥"]
            if any(word in title for word in pro_filters):
                articles.append({
                    "title": title,
                    "link": link,
                    "date": TODAY_STR,
                    "source": "æ—¥æœ¬æ¥­ç•Œå‹•æ…‹"
                })

            if len(articles) >= 10: break
                
        print(f"âœ… ç¯©é¸å®Œæˆï¼Œå…±æ‰¾åˆ° {len(articles)} å‰‡å°ˆæ¥­æ·±åº¦æ–°èã€‚")
    except Exception as e:
        print(f"âŒ æŠ“å–å¤±æ•—: {e}")
        
    return articles
#3.5 è«–æ–‡å®šæœŸæ›´æ–°
#
#
def get_journal_papers():
    journals = [
        "Journal of Financial Economics", "Journal of Banking and Finance", 
        "Journal of Corporate Finance", "Journal of Japanese International Economy",
        "Journal of Money, Credit and Banking", "Journal of Financial and Quantitative Analysis",
        "Review of Financial Studies", "Journal of Risk and Insurance", "Insurance: Mathematics and Economics"
    ]
    
    all_papers = []
    print(f"ğŸ” é–‹å§‹ç›£æ¸¬ {len(journals)} æœ¬é‡‘èé ‚åˆŠ...")

    for j in journals:
        rss_url = f"https://news.google.com/rss/search?q=intitle:%22{j.replace(' ', '+')}%22&hl=en-US&gl=US&ceid=US:en"
        
        try:
            response = requests.get(rss_url, headers=HEADERS, timeout=15)
            soup = BeautifulSoup(response.text, "html.parser")
            items = soup.find_all("item")

            count = 0
            for item in items:
                title_tag = item.find("title")
                link_tag = item.find("link")
                
                if title_tag and link_tag:
                    raw_title = title_tag.get_text()
                    clean_title = raw_title.split(' - ')[0].split(' | ')[0]
                    
                    # --- ç¸®æ’ä¿®æ­£å€é–‹å§‹ ---
                    raw_link = link_tag.get_text()
                    if not raw_link or "http" not in raw_link:
                        if link_tag.next_sibling:
                            raw_link = str(link_tag.next_sibling).strip()
                        else:
                            raw_link = ""

                    if "http" in raw_link:
                        all_papers.append({
                            "title": clean_title.strip(),
                            "link": raw_link,
                            "journal": j,
                            "date": TODAY_STR
                        })
                        count += 1
                    # --- ç¸®æ’ä¿®æ­£å€çµæŸ ---

                if count >= 3: 
                    break 
            print(f"âœ… {j}: å·²æŠ“å– {count} ç¯‡")
        except Exception as e:
            print(f"âŒ {j} æŠ“å–å¤±æ•—: {e}")
            
    return all_papers
# =========================
# 4. åŸ·è¡Œèˆ‡å„²å­˜
# =========================
def main():
    # 1. ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)

    # 2. åŸ·è¡ŒæŠ“å–
    tw_news = get_taiwan_news()
    jp_news = get_japan_news()
    papers_list = get_journal_papers()  # æ”¹åé¿å…æ··æ·†
    
    # --- è™•ç†æ–°èè³‡æ–™ ---
    all_news = tw_news + jp_news
    if all_news:
        df_news = pd.DataFrame(all_news).drop_duplicates(subset="title")
        all_news = df_news.to_dict('records')
    else:
        all_news = [{
            "title": "ä»Šæ—¥æš«ç„¡æ›´æ–°æ–°è",
            "link": "#",
            "date": TODAY_STR,
            "source": "ç³»çµ±è¨Šæ¯"
        }]

    # å¯«å…¥æ–°è JSON
    news_json_path = os.path.join(OUTPUT_DIR, "news_data.json")
    with open(news_json_path, 'w', encoding='utf-8') as f:
        json.dump(all_news, f, ensure_ascii=False, indent=4)
        
    print(f"âœ… æ–°èæ›´æ–°å®Œæˆï¼å­˜å…¥ {len(all_news)} å‰‡ (å°:{len(tw_news)}/æ—¥:{len(jp_news)})")

    # --- è™•ç†è«–æ–‡è³‡æ–™ ---
    if papers_list:
        # è«–æ–‡ä¹Ÿåšå»é‡è™•ç†
        df_papers = pd.DataFrame(papers_list).drop_duplicates(subset="title")
        papers_final = df_papers.to_dict('records')
    else:
        papers_final = [] # å¦‚æœæ²’æŠ“åˆ°å°±ç•™ç©ºåˆ—è¡¨

    # å¯«å…¥è«–æ–‡ JSON
    paper_json_path = os.path.join(OUTPUT_DIR, "paper_data.json")
    with open(paper_json_path, 'w', encoding='utf-8') as f:
        json.dump(papers_final, f, ensure_ascii=False, indent=4)
        
    print(f"âœ… è«–æ–‡æ›´æ–°å®Œæˆï¼å…±å­˜å…¥ {len(papers_final)} ç¯‡è«–æ–‡ã€‚")

if __name__ == "__main__":
    main()
