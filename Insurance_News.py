import requests
from bs4 import BeautifulSoup
import pandas as pd
import datetime
import os

# =========================
# 1. åŸºæœ¬è¨­å®šèˆ‡è·¯å¾‘ (GitHub ç’°å¢ƒå°ˆç”¨)
# =========================
# ç¢ºä¿è³‡æ–™æœƒå­˜åœ¨æ ¹ç›®éŒ„ä¸‹çš„ data è³‡æ–™å¤¾
OUTPUT_DIR = "data"
if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)

KEYWORD = "ä¿é™º"
# ç¶²é é¡¯ç¤ºç”¨çš„æ—¥æœŸæ ¼å¼
TODAY_STR = datetime.datetime.today().strftime("%Y-%m-%d")

# =========================
# 2. æ–°èçˆ¬å–å‡½æ•¸
# =========================
def get_nikkei_news():
    url = f"https://www.nikkei.com/search?keyword={KEYWORD}"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }
    try:
        response = requests.get(url, headers=headers, timeout=15)
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.text, "html.parser")
        articles = []
        
        # çˆ¬å–é€£çµèˆ‡æ¨™é¡Œ
        for a in soup.find_all("a"):
            title = a.text.strip()
            link = a.get("href")
            
            if link and link.startswith("/"):
                link = "https://www.nikkei.com" + link
            
            # éæ¿¾æ¢ä»¶ï¼šæ¨™é¡ŒåŒ…å«é—œéµå­—ä¸”é•·åº¦åˆç†
            if KEYWORD in title and len(title) > 5:
                articles.append({
                    "title": title,
                    "link": link,
                    "date": TODAY_STR,
                    "category": "æœ€æ–°å‹•æ…‹"
                })
        return pd.DataFrame(articles)
    except Exception as e:
        print(f"âŒ æŠ“å–éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
        return pd.DataFrame()

# =========================
# 3. åŸ·è¡Œä¸»æµç¨‹
# =========================
def main():
    print(f"ğŸ” é–‹å§‹æŠ“å–ã€Œ{KEYWORD}ã€ç›¸é—œæ–°è...")
    df = get_nikkei_news()

    if df.empty:
        print("âš ï¸ æœªæ‰¾åˆ°ç›¸é—œæ–°èï¼Œè«‹ç¨å¾Œå†è©¦ã€‚")
        return

    # ç§»é™¤é‡è¤‡æ¨™é¡Œ
    df.drop_duplicates(subset="title", inplace=True)

    # è¼¸å‡ºç¶²é ç”¨çš„ JSON æª”æ¡ˆ
    json_path = os.path.join(OUTPUT_DIR, "news_data.json")
    df.to_json(json_path, orient="records", force_ascii=False, indent=4)
    print(f"âœ… JSON æª”æ¡ˆæ›´æ–°æˆåŠŸ: {json_path}")

    # åŒæ™‚ç”¢ç”Ÿ Excel å‚™ä»½ä¾›å­˜æª”
    excel_path = os.path.join(OUTPUT_DIR, f"news_log_{TODAY_STR}.xlsx")
    df.to_excel(excel_path, index=False)
    print(f"âœ… Excel å‚™ä»½æ›´æ–°æˆåŠŸ: {excel_path}")

if __name__ == "__main__":
    main()
